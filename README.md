# American-Sign-Language-Detection-System
## Project Overview
This project aims to develop a sign language detection system using computer vision and machine learning. The goal is to recognize and classify hand gestures, enabling communication for individuals with hearing impairments. By utilizing a trained machine learning model, the system identifies gestures through a camera feed, allowing real-time sign language detection.

## Problem Statement
With the increasing need for accessibility in communication, this project focuses on solving the problem of interpreting sign language. By leveraging computer vision and machine learning, this system can translate hand gestures into meaningful text, bridging the communication gap between individuals who use sign language and those who do not.

## Technologies Used
- **Programming Language**: Python
- **Libraries**:
  - OpenCV for image processing and video feed handling
  - TensorFlow/Keras for deep learning model development and prediction
  - cvzone for hand tracking and gesture classification
  - NumPy for numerical operations

## Features
- Real-time hand gesture detection using a webcam
- Classification of multiple sign language gestures
- User-friendly interface for displaying predictions on the screen
- Customizable model to include more hand gestures
- Pre-trained model for accurate gesture classification

## License
This project is developed by Rohit sah. All rights reserved. No part of this project may be used, modified, or distributed without explicit permission.
